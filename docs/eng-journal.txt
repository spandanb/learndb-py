Engineering Journal
-------------------

5/20
    context: thinking about interface/api for cursor

    thoughts: who should be responsible for storing.
pager handles interaction with pages
cursor handles interfacing with table, and understands table position
    i.e. where to insert/read from, and how to traverse table
table: right now functions as the engine, since it handles interacting with the storage layer
    to support multiple tables, storage access, would have to be moved to engine
    then table will primarily represent the logical operations on the table

consider what the ideal interface between Table, Engine, Cursor, Pager should be

5/24
    translating c tutorial, get's tricky because pointers allow for a different interface
    than pythons reference. This does bring up the point about what is a correct modelling
    of the underlying exchange.
    Specifically, should a btree have a higher up interface?


6/4
    One challenge is inconsistencies in your logical flow. The compiler/interpreter/language will
    enforce and offer some framing. But all sufficiently complex applications will break out of this box. This box
    being, the subset of the ways your code can be incorrect that can be caught by compiler. But once you're concerned
    about the flow values, or rather what states of program are valid or not.

    All interface design- e.g. the breakdown of logic into components like functions, classes, methods,
    i.e. not just interface in the OO interface, or interface in a specific language, but rather, in the broadest sense,
    the communication patterns of all modules in your program- can be thought of as designing a language- a DSL.
    This metaphor of interface as language, allows us to reason about incorrectness in code in terms of undefined behavior
    of the language. And this is what all manners of tests (unit, functional, end-to-end) are attempting to do, ensure
    that the program doesn't invalid states/"undefined behavior".

    Mostly, this interface, this DSL is not explicitly defined.

    One example of the above challenges, is working with pages of bytes in the context of btree. Now, since I'm reading
    writing subsets of bytes to larger byte strings, the interpreter can't directly help. And I need to either implement,
    a better lower level DSL, i.e. one that supports better debugging.

6/9
    When the codebase gets complex, all interfaces will show their leakiness. Leakiness will manifest in wanting to
    expose internal variables (e.g. propagating max key value when max key inserted in leaf up internal node ancestors;
    if the max value is not propagated, the internal node will have to look it up, which would be a replicated op- and this
    also speaks to the broader compute/storage tradeoff. ). The options are these: 1) break the abstraction, expose
    internals- makes the code brittle; 2) keep the abstraction, make the code less performant, 3) rethink the abstraction
    such that the internal to be exposed is a first-class exposed objects- more work, but keeps the code performant and
    abstractions sounds.

6/10
    I missed a crucial detail in my modelling of the insert algorithms, since I assumed some things about how a node
    would be split, and how the post-split key distribution will look like; this meant a lot of debugging. This is
    why it's very important to independently reason about the algorithm. Because, I kept persisting in that modelling error
    and so in the program all that was left to was fix the control flow, the +1 errors.

6/10
    There's a critical stress between core developement and auxilary tools development, i.e. broadly infra. Core is
    fundamentally the algorithm of the business. If it involves humans, it maybe better thought of the aggregate flows
    of the business. And there are things that don't have a direct value add, but are needed, e.g. testing. It's not
    a direct value add, but it is an indirect value add in reliability.

    But the infra should involve with the complexity of the core, e.g. insert keys into tree in sorted order;
    small tree - manually inspect state of tree is feasible, for medium tree- tests on expected values, for large trees-
    internal consistency checkers, combined with random tree generators. Here sizes are metaphors for complexity.

6/11
    An off by one error fixed here, and an inequality fixed there, and poof, you have a (semi-) working btree (only find, and
    insert; no delete). It's cool, how it all comes together. It's a very fascinating exercise to work with this pure
    level of abstraction, i.e. just the raw data structure, and some algorithm applied thereon. Aside about how complexity, and
    friction in the context of code arises: complexity arises either due to the inherent nature of the problem, or it arises
    due to the complexity of "integrations". This pure level of abstraction is where complexity arises entirely from the problem.
    Because there is no integration, e.g. the btree with 2000 lines has no non-system imports. And it's certainly a good
    thing to exercise your brain thus.

    The next step before implementing anything else, would be some code hygiene, e.g. move tree to a separate module. Assess,
    if any logic should be refactored, api's cleaned up. Add any more validations.

    Actually, the actual next step should be to reflect on whether to work on delete, learndb-rs, debugging/inner-loop.

6/17
    There are two crucial aspects to testing. The first is given this input, my system should return this output. And we
    can think about these input/output, more abstractly to include metrics like SLA, e.g. the calculator should solve 2+2
    in under 10ms. These are the known knowns. The second aspect of testing, is the known unknowns, and the unknown unknown.
    A known unknown maybe the exact concurrency limit (curve) of a system. The known unknowns are the un-measured, or the
    un-modelled aspects of the system. These could with more effort be made more known, but never perfectly so.
    A unknown unknown is a completely unknown behavior of your system. But it's always grounded in some assumption. In the
    most extreme case, consider a beta-particle hitting your memory, and flipping a bit or two in an undetectable way. Now
    this is a very rare, scenario, but your assumptions that integers are immutable is valid insofar, as your physical
    memory is perfectly isolated.

    This distinction is important, because the unknown aspects of the systems, can cause failure. One way to discover the
    unknowns is through randomized searches, through your program's state space.

6/28
    A case study in evolving code. I initially started with the assumption that trees can never be unary. This simplified
    some operations, e.g. find; and it was not possible for a (sub-)tree to become unary from insert ops. When I
    started implementing delete/restructure- I faced the choice of either allow unary trees and update the existing ops
    to handle a unary tree. Or guard against the tree ending up in such a state. I opted for this initially because
    it seemed easier to restrict the tree not becoming unary. But then this started getting more hairy; for example- some
    delete ops, could lead to unary node; and you would need many conditions in delete; restructure; and find and possibly
    even some rebalancing (e.g. if a node's child cells are all deleted).

    One take away from this is that software that's less piece-meal and more of a consistent and well-thought abstraction is
    more likely to generalize in it's use cases; and in likely to be less complex in its logic.


6/29
    I'm doing a fairly crude compaction (left most; without any buffer). In practice, this should be configurable, and
    perhaps at least not so eager.

7/2
    internal_num_keys vs. internal_num_children.
    internal_num_keys is the accessor for the number of keys on an internal node; This is very similar
    to leaf_num_keys; with the crucial difference, that internal node have +1 (right) children than keys, while leaf
    node have the same number of cells- and this is because the two node types have different functions.

    This makes delete tricky, because it makes the interpretation of number of children on a node, ambiguous. This
    itself is related to the underlying modeling of the tree, and whether unary is a valid tree etc. Specifically,
    internal_num_cells == 0 could be a unary or zeroary tree. So delete has to track the state of tree before
    and determine which of these two buckets it's in. A clearer way would be to use a new accessor internal_num_children.
    Ideally, this would have include the total count; and could easily disambiguate unary and zeroary calls. But this
    is where the ideal design runs into the weight of the existing work; specifically, my options are: 1) introduce a new field-
    wastes space; 2) update internal_num_keys to internal_num_children but this is tricky because its a very commonly used
    method, including in find, binary search etc; but anyways this is not an excuse;

    The point being, somewhere in the deep of the mist, you will realize the weakest aspects of your design.

    Anyways for now, will opt for contextual delete, i.e. no new fields?

7/5
    One challenge of memory vs on-disk data structures is that all memory protections, etc. are gone, since they
    only exist for in memory object. For on-disk objects, all of that: memory management, resource management, has to
    be one by oneself. And more importantly, it depends on the correctness of other parts of the system.


11/17
    PEGs vs hand-written recursive descent parser
    Working on extending the database to support a more complex grammar/DDL/DQL/DDM to support more complex functionality,
    e.g. filtering on search. Initially, I experimented with a PEG (parse expression grammar) library. PEGs are
    quiet expressive, and easy to use. But PEGs are limited since they generally expect the input to be correctly
    formatted- and as such aren't very good at reporting errors, and more broadly, doing meaningful things with
    invalid input.

    Then I created a recursive descent parser using the skeleton from crafting interpreters by Robert Nystrom.

11/18
    Python vs. statically typed language
    I started with python because it seemed the fastest. But now given the complexity, and need to refactor code- I feel
    a static language would have been better. Actually, this is more nuanced- python is useful in bootstrapping really
    quickly- 2 lines for a repl. And a lot of the benefits of statically typed languages can be simulated to
    a certain level, e.g. mypy + type annotations ~ static type system. But it's certainly less than a industrial grade
    statically typed language, and this seems to be made worse with the advantage statically typed language in IDEs.
    Again, there seems to be much advancement in editors- but even things as refactor don't feel as fluid.
    Will stick with python (or perhaps later migrate some stuff to a diff lang) But something to reflect on.

11/18
    I'm trying to add support for some DDL/M/Q support. What I have so far can be considered a template. It's functionality
    is the ability to store integer keys and support efficient  ops (find, insert, delete). Next, I want to support dynamic,
    multiple tables. This will require a host of changes, across the entire project.
    The first step of this is the parser, which is complete. The details are in documented in
    docs/to_multiple_dynamic_tables.txt


11/23
    In trying to support arbitrary schemas, one must consider how a logical schema is represented; and how are
    ops on schemas supported. This includes validation and generation of schema.
    One must also consider the type system and how datatypes are implemented and manipulated
    at runtime and stored on disk. This also is a can of worms, because they're are many tradeoffs around
    how the data is serialized (e.g. fixed length vs. variable length encodings). Another concern is, whether
    null values are explicitly stored, i.e. sparse vs non-sparse representations.

    Actually, we need to store the schema anyways, since any type can be null'ed. See (docs/storing-diff-datatypes.txt)
    for more details.

    Further deep, there may be tradeoffs around consistency/validity/expressivity of the type system.

11/24
    The serialized bytes (persisted to disk) need to encoding many details, e.g. column data type etc. This information
    goes in the header. The btree module was doing header read/write by referring to an implicit header defined via
    constants. This was quite manual.

    I am considering a higher level interface for reading, writing the header. This would also make it easier to do
    validations and change constants. Ideally the interface is also self documenting.

    e.g.

    Header has many fields.
    Each field has:
        - a name (e.g. is_root_node), and
        - value (which can be optionally mapped to an enum type)
        - repeat count [0, inf]

    header = Header()
    # add a new field, with a given size
    header.append_field('header_size', type=int, size=4)
    # add a new field that's repeated
    header.append_field('serial_type', type=SerialType, size=4)

    # alt. syntax
    header_size = Field(name='header_size', type=int, size=4)
    stype = Field(name='serial_type', type=SerialType, size=4)
    var_len = OptionalField(name='length', type=int, size=4, read_if=lambda st: st.value.type = Text)

    if stype.value
    header.append(header_size)

    stype_len_pair = Field(Field(name='serial_type', type=SerialType, size=4),
                            Field(name='length', type=int, size=4, repeater=ZeroOrOne), repeater='', repeat_until='end')
    header.append(stype_len_pair)

    # the header should return a parser
    parser = header.get_parser()

    => {header_size: 4, columns: [{serial_type: 4, length: 32}] }

    ---

    What's peculiar here is that we end up with another implicit parser/language. This is distinct from the sql parser, i.e.
    the front end. This is a parser for the contents of the on-disk node. What's common between the 2 languages is that structure
    of the tokens encodes the meanings of the tokens. I suppose any time we have this- we need a parser.

    ---

    Actually, after experimenting a bit, it seems this header handler that can simultaneously be a self-documenting header
    definition as well as provide an auto-generated parser- will be quiet difficult to implement.

11/26
    Logically segmenting components is tricky. The first level division is between stateless and stateful
    ops. This is broadly, the split between virtual machine and state manager. But there are a lot of boundary concerns, e.g.
    how a record should be created. Further, there is a fundamental tradeoffs between encapsulating state (object-oriented
    perspective and clearly expressing compute on some data (functional perspective).

    One case in point is who should be responsible for initializing the catalog. At one level, I want the VM
    to handle all reads/writes since logic will be tricky and best contained in one object. However, then the
    VM would have to intimately understand the catalog format, and this would make the VM fatter.

11/28
    See discussion on updating btree to support variable length cells in docs/btree-variable-length-cells.txt

11/29
    Incrementally evolving software

    One issue I'm running into is whether the API around leaf nodes should expect a key of type integer or bytes.
    The reason is, for now I'm doing word-sized integer keys. But eventually I will want to support variable length integer
    keys (any byte string can be interpreted as a multi-bytes int). So there is some api (new, forward looking for leaf nodes)
    which expects byte strings as keys. And older api (which I'm trying to reuse for internal nodes).
    But mixing the old and new apis can make the code confusing. But the alternative is either:
    a much bigger one-shot change, i.e. update old reuseable api to expect bytes, or a slower two step update, i.e.
    support new file format with fixed sized ints. Then update to support variable sized ints.

    But this is just a place where competing objectives collide- time, code clarity/correctness. These two are not necessarily
    in conflict. Higher code clarity, makes it less like to have bugs, and easier to debug, and you need a minimum level of code
    beauty (i.e. everything that's not raw functional, i.e. the difference between a turing machine and a slick interface). But
    beyond a certain level- the gain in productivity is limited.


12/11
    serde.py::get_cell_key(page, ) vs. btree knowing how to read key from a cell
    the latter diffuses the logic for reading cells and leads to multiple reader implementations.
    the former, requires passing the entire page, since I can't create a referential slice on a subset of the page.
    this is problematic because there is a higher risk of corrupting a page due to errors.

12/11
    Another issue is that of centralizing the serialization, e.g. of ints. This happens in 2 place, serialization in the
    context of records to be inserted. And in serialization of header field values for the btree. Right now, my inclination
    is to keep them separate, since int serialization is one line. but, the argument is not about the amount of code, but
    about the level of abstraction that is exposed. and I think the btree as a manager of bytes, should control- at least
    in its headers- how these bytes are interpreted.
    And in some cases, the btree may want to opt for non-standard int impls e.g. smaller or larger than the default- types
    which may not make sense in the user accessible type system- since that type system will have a broader surface area, e.g.
    associated serial-types.

12/11
    Now consider select statements- how should results be piped. I like the idea of having a separate pipe like object that can
    be used read the results as needed. The other option is just return a materialized list of results. Will do the latter for
    the speed, and can consider the former latter.

12/12
    How strict should the type system be? Right now the tokenizer reads all numbers as floats. But this causes
    problems downstream in the vm- since the type system is rigid and reading a integer valued float
    raises a value validation error. For now will update the tokenizer to distinguish floats and ints.

12/14
    Case sensitivity of identifiers. Right now only unquoted identifiers are supported. Identifiers are case insensitive
    and internally represented as their lowercased version.

12/16
    As I'm rewriting btree insert logic to handle variable length cells, I opted to rewrite some of the in-place operations
    out of place. e.g. when a node split happens, the in-place way is that node being split is reused as one of the splits;
    the out of place approach is to create a separate node for the split. The latter simplifies the logic, both because:
    1) you don't need to do any bookkeeping around what space you can touch and what you can't; 2) you can simultaneous access
    before and after states.

12/18
    Btree in-place vs. out-of-place splitting.
    One of the trickier aspects of the btree implementation is the split operation (and compact) operations.
    Previously, when cells were fixed-length, this op was done in-place, i.e. the node that was being split, would
    become one of the future splits. This is tricky with variable-length cells, so I opted for out-of-place splitting.

    But internal nodes can be split-in place with not too much difficulty. But then that requires supporting two mechanisms/
    methods for internal_node_split- one that expects the old_node to be recycled- i.e. when called from a leaf. And
    another that expects the old-node to be one of the new splits - i.e. when called from another internal node operation.

    It's simpler to support out-of-place ops- even though this translates to obviously poorer performance (due to
    more copy ops- not empirically validated). Although this op, should happen rarely on a full btree- so the amortized
    cost should be low.

12/27
    Some reflections on sub-iterations vs. mapped iterators
    Context: btree internal_node_split_and_insert
        #currently we iterate over all children- internal and right
        # when it sees the child_num of old_child - it starts a
        # sub-iteration over the new children.
        # sub-iteration over the new children.
        # The alternative to sub-iterators, is to materialize
        # sub-iterator into mapping of each src child and dest node and pos, e.g.
        # shifted_cell in prev impl of internal_node_split, which encodes this location,
        # although in a way less comprehensible than nested loops

12/28
    Two key decisions I want to document here:
    First, w.r.t. the btree, all structure changing ops will be out-of-place, e.g. a split op will not reuses
    the node that was split- as before, but provision new nodes. The main reason for this is simplicity. In-place ops
    tend to be trickier, since you need to be mindful of what already exists. On the flip side, the performance cost
    seems low, since on a btree this should happen rarely. And perhaps, there may be some gains, e.g. under weaker
    serializability levels.

    Second, free list is unsorted; later I can get fancy with sorting the list, but for now this is good.

1/16
    A lot of records objects are being created, e.g. when doing joins. Also, the joins are implemented
    via scans over the entire joined record sets. These result in high space and time usage respectively. Some of
    this usage could be reduced; but the code would get trickier. Right now, I'm optimizing for code completion and
    correctness- space and time concerns should be addressed after running proper benchmarks; and only once I know
    what correct code and behavior looks like. Further, if I do end up writing more optimized handlers, I will have
    a reference implementation - which while inefficient should be easier to reason about.

1/20
    Joins are implemented as nested loop. This works for inner join. To handle outer join, I'll need to use extra
    space to track what rows have been joined. Specifically, consider, right outer join.
    I'll keep track (via a seen index) of rows from the right source (inner loop) that have not been joined, and hence not added to
    the result set. Then after the nested loop, I'll iterate over the seen index and add null'ed copies of rows to the
    result set.

1/21
    If-else and dumb objects vs. conditional flag aware objects

    Anytime I have an if-else branch, I can replace it with a data-structure, that can handle the condition. This is the
    functional - object split. Specifically, this is a concern in the handling select statements, since there are many
    stages, e.g. from, where, group by,... and each stage, modifies the result set of the previous stage. Initially, I was
    preferring iterating, since it keeps the data structures dumb iterates. But, group by splits the result set into multiple
    grouped sets. And using the above approach would require iterating over each group, to apply filters and aggregates, and
    reconstruct a new group after- and this looks and feels quiet ugly. So, here I'm leaning towards having the ResultSet
    and GroupedResultSet, have truncate and aggregate methods- i.e. the data structures are not so dumb.

1/27
    Extending parser and virtual machine to support all clauses

    I'm working on extending the parser and VM to support group by, having, limit, clauses, as well as nested select
    statements. This will rework the parser to have a few more symbol types. Also, the VM handler methods will be updated.

2/20
    Finally got lark to work. My language had various types of ambiguities and left and right recursions. Changing, those,
    and broadly, rethinking how to construct the grammar such that these scenarios are avoided helped. However, unlike
    the hand-rolled recursive descent, where I could do some tran-parser processing, e.g. attach joinType enum based on
    the modifier to join keyword, I will need a secondary indirection, either as a layer of processing, that
    transforms the tree, or a wrapper around the parsed tree, that enables retrieving fields.

2/27
    Still had some issues with the language parser, but looks like it's finally works. Now, trying to convert parse
    tree into an Ast, ie. only with the desired classes in the hierarchy and string converted to literal values.

2/28
    Some ideas on how to organize vm::select_stmnt logic; VM will have method to help construct the RecordSet
    and GroupedRecordSet. This way, the logic for interpreting a stmnt would not have to be duplicated in vm
    and recordSet. Here the record set will be dumb. This makes the VM explicitly single threaded.

    Joins will be implemented as double-for loop. There maybe some optimizations here, e.g. avoiding full scans
    when doing an equi-join; but for now I will pass on that. Later, I can revisit this and apply optimization.


3/21
    Constructing the AST using Lark classes leads to issues when rules have optional symbols which are not
    at the tail. Thus, the tokens are assigned to the right instance vars. One fix is to accept names as pos args,
    while also requiring a user defined mapping of required args and types they could assume. e.g.
    tokens = [
        left_src: Field(Optional=True, types=Union[Joining, SingleSource]),
        ...
    ]
    The other way to avoid this is to write the grammar in a way that rules don't have intermediate optional.
    The third way, and the one I've opted for, is to use lark's ast construction utilities for rules, which
    don't have optional terminals in body of the rule. For rules which have optional in the body, I explicitly
    resolve the field names with the right arguments. This "resolution" requires me to specify the rule_name or
    the ast types an arg could be. The resolution also fails e.g. for cases with duplicated symbols, where one of those is
    optional. But these can be avoided.

4/16
    I've been spinning on the most manageable parser solution. Lark's utilities for transforming parse tree to AST are
    somewhat lacking; e.g. it can't assign rules based on kwargs for an AST's init method. Actually, the reason it doesn't
    is that it's solving a more general problem, and specifically, you can have a rule where a symbol is repeated twice. This
    can't be mapped to an init. However, I have written, and I suppose a simple enough grammar like sql can be written to avoid
    this issue.

    Anyways, there were 2 problems, one of optional's in the middle of a rule body- this is solved by mapping the rule names to
    init kw args, manually in the transformer class. The second is of repeating rule_names- this can be solved by defining an alias
    for a symbol, i.e. the write the grammar in a way that avoids this issue. This is not the purest solution- but again, this parsing
    business is a can of worms for an imperative programmar.


5/14
    Is the catalog a table, that must be protected, i.e. can't be dropped unless whole db is dropped. Or is it something special,
    i.e. not a table. This distinction is important, because it affects how we think about the VM apis. i.e. do we use
    the same api/functions to operate on table and catalog (with if conditions), or do we think about them as fundamentally
    different types of objects and hence hitting different flows (albeit, common helper functions can still be shared).
    The above point is a broader point, about what are the core functionalities and APIs that, e.g. the VM operates on.

5/27
    Generating the AST from text has been the biggest tailspin for the last little bit.
    This is how the parsing evolved
        - static parsing, e.g. only parse primary key
        - handle rolled recursive descent parser;
            - this had a lexer, and a parser. However, my grammar was poorly written with
              many ambiguities. To resolve the ambiguities, I had to understand how
              to write the grammar without ambiguities. When I had spent enough time iterating on this,
              I didn't care for a hand- rolled RD parser, and so I went looking for parser generator, to automate this.
        - experiment with lark, pyparsing; opting for lark
        - lark, use to_ast helper
        - this has problems when trying to handle rules with optional symbols not at the tail,
        - simultaneously, I want to automate generating the AST classes for to_ast, from the grammar spec
        - this eats a lot of time, i.e. to first parse the grammar file itself, then trying to use lark, itself
          to parse the lark grammar file
        - to avoid this, I try to store the rule_name (which is included in the parse_tree node) but not in
          the AST node, since that's generated by me. I also try to create a map of rule_name/ast class name
        - to keep the rule_name, I use @v_args(tree=True), but this makes unwrapping the tree object tricky,
          and after a few itertions of unwrapping all trees, it still doesn't work.
        - the above approaches could have worked, but they each have their own edge cases, and introduce more complexity
        - Custom transformer, where I handle AST node generation via handlers. These handlers unwrap any nodes etc,
        - Ultimately, this is the approach that worked.

    Note, the current approach essentially requires 3 different locations where a symbol is defined. Once, in grammar,
    then in the transformer method for the rule, and finally in the AST symbol class. Albeit, the mapping between grammar
    rule and AST symbol class is not 1:1, hence the 3 definitions, which allow the arbitrariness of mapping.
 
7/12
     How should scoping and name aliasing in leardb-sql work. Currently, scope alias info is stored in multi-record.
     And single-source recordset, don't have any alias info. Specifically,

        select *
        from foo f
        join bar b
         on f.x = b.y

       This stores recordset, like {'f': FOORECORD, 'b': BARRECORD}. i.e. the multi-record stores the alias mapping.
       ...

       Right now I was able to get the aliasing to work; but it doesn't seem very elegant. And this inelegance is likely
       hiding inconsistencies; and bugs. At some point I should revisit, how table_name aliases are handled. Some things,
       to consider, is that Should I keep the mapping externally, e.g. in vm::current_scope? Here, external, means
       external to the record/recordset.

       Another way to consider this is, how should we handle table aliases in a select with only 1 source. This is only
       an issue for 1 source, since a multi-record stores the name mapping. One solution would be to rewrite the query
       and drop any alias e.g. select * from foo f group f.x -> select * from foo group by x; or I could do the reverse;
       or I could handle the two as separate cases.

 7/15
     Right now, group by and non-group by cases are handled separately. But actually, a non-grouped select, can have aggregates
     performed on it, which behave like an aggregatable group.

 7/23
    Implementing the vm::select_clause, i.e. taking a (un)grouped set and materializing the select clause. Which can
    contain functions. This brings the question of how should the type system be like? And how are functions written?
    What is the type signature of max? max(Int)? what about max(Real). Then I am implementing polymorphism?

8/6
    Datatype
    There are 3 datatypes: 1) the storage type, 2) the in-memory type, 3) the parsed type symbols.

8/14
   When evaluating select_clause, I need more than what the output column looks like, but how will
    output column be generated; i.e. a output column generator.

8/14
    One beautiful thing in this project is the recursive structure, occurrence of certain properties. For example,
    even inside the VM, internal methods interact via an API over recordset; and never on the raw recordset object.
    Even here, having this level of abstraction is helpful; because it allows for evolving independent code more easily.

8/18
    How to model functions?
    There's 2 components: the function that operates over datatypes, and how the record value is extracted to pass to function.
    Thinking through function evaluation, I need to rethink first how expressions are evaluated, e.g. select 1+1
    The grammar must be updated to support expressions. An expression evaluates to a value.

9/18
    Refactor the grammar to support functions calls and more as (implicit) exprs. Implicit, because I don't define an
    "expr" symbol- condition currently being the root of the expr hierarchy.
    Returning now to input-to mapping. The idea is we'll iterate over selectables in select clause and construct a generator
    objects, which can be passed a record and can generate a record in the output format.

10/10
    The next bit is to model function: 1) definitions and 2) invocations/call. Minimally these will need to have a name,
    parameters/arguments. Currently, I will limit it names defined inside the function, or a parameter, i.e. no non-local
    names; later this can be revisited.

10/31
    Going down the function modelling rabbit hole:
        - do functions operate on explicit types, e.g. real, int or abstract types like numeric (cosnider the average function)
        -- for simplicity opting for explicit mapping, i.e. avg(x: int, y: int), avg_real(x: real, y: real), rather than polymorphism
        - types, so far the official supported have been integer, float, string, blob, i.e. storage layer types
        - and all other types map to these types, e.g. ast_types
        - but functions may need to operate on arrays, and other types which will never be directly stored, but are
          useful to operate on,

10/31
    Enumerating the components of a select clause
    e.g. select cola..., select max(cola)
    First we determine whether the clause is operating over a recordset or groupedrecordset
    Then we create ValueGeneratorFrom{Scalar, Vector} which holds references to symbolic names, e.g. cola.
    Then from the valueGenerator object, we pass it a record (or grouped recordset)

    output_generator = []
    for selectable in select_expr:
        output_generator.append(ValueGeneratorFrom(selectable))

    output = []
    for record in recorset:
        out_rec = {colname: value_gen.get_value(record) for colname, value_gen in output_generator.pairs}
        output.append(out_rec)


    Now let's consider the interface, and impl of a value generator

    ScalarValueGenerator()
        init(apply_fn(record)
        get_value(record) -> value
    VectorValueGenerator()

    NameResolver => maps col.name to function positions?

    There is some overlap between FunctionInvocation and ValueGenerator

11/19
    Two documents
     1: select-clause-evaluation elaborates on how entire select clause eval pipeline looks.
     2: functions.txt documents how functions are defined, eval'ed etc


12/3
    Monolithic VM
    I started with a fairly monolithic VM, where all functionality was handled directly in VM class.
    This approach works for the btree, because: the logic is more contained, i.e. has far less branching, 2)
    far fewer total possible call paths, i.e. you can go down the insert -> split leaf -> split ... internal -> root,
    or you can get all keys, or you can go down the delete -> ... recombine hierarchy.
    Compare this to the VM, where each clause e.g. from, has alone more call path complexity than all of the btree.
    Long story short, this is in need of refactor.
    How?
    One approach would be to split it along clause, e.g. select clause, from_clause, where_clause.

12/14
    How to do name resolution? e.g. to determine output schema column type, need to lookup column.
    One approach is to implicitly keep track of context, or scope. A scope is a logical environment, where names, e.g.
    column names, function names, sub-query etc are defined. Scopes can be nested. There will be a global scope, which
    will contain function names, etc. Next, each query will create its own scope. And a sub-query will create a further nested scope.
    But this also means that:
      1) function resolution will have to be merged with general name resolution.
      2) scoped schema name management will have to be merged with the name mechanism

    For now, I'll leave the other name resolutions as it. But this should be refactored at some point

    What is the API for managing scope
    init_scope()
    register_scoped_object('name', object, type?)
    resolve_name('name') -> (object, type)

12/15
    The VM kind of implements the visitor pattern. But instead of having one method per symbol type, I've crammed, entire
    expr evaluation in one method. This is leading to a lot of duplication. Perhaps, I should clearly have method that can
    walk the expr hierarchy.

    Revisiting the VM.
    I need to think about all the components that the VM will need.
    Specifically, I'm currently missing a semantic analysis layer;
        - this should check all expressions are as expected,
        - maybe simplify expressions,
        - and bind names to objects, e.g. select column_name to actual column

    Also, the VM needs to be split into smaller modules.

    I'm thinking, to create module for each statement type, e.g.
    create_statement_handler.py
        - which would mostly be stateless, so could be represented as stateless functions, or an object
    the VM would hold all state, e.g. state_manager, scopes, recordsets, etc,
        and would expose them to the sub-modules as needed
