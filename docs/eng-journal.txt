Engineering Journal
-------------------

5/20
    context: thinking about interface/api for cursor

    thoughts: who should be responsible for storing.
pager handles interaction with pages
cursor handles interfacing with table, and understands table position
    i.e. where to insert/read from, and how to traverse table
table: right now functions as the engine, since it handles interacting with the storage layer
    to support multiple tables, storage access, would have to be moved to engine
    then table will primarily represent the logical operations on the table

consider what the ideal interface between Table, Engine, Cursor, Pager should be

5/24
    translating c tutorial, get's tricky because pointers allow for a different interface
    than pythons reference. This does bring up the point about what is a correct modelling
    of the underlying exchange.
    Specifically, should a btree have a higher up interface?


6/4
    One challenge is inconsistencies in your logical flow. The compiler/interpreter/language will
    enforce and offer some framing. But all sufficiently complex applications will break out of this box. This box
    being, the subset of the ways your code can be incorrect that can be caught by compiler. But once you're concerned
    about the flow values, or rather what states of program are valid or not.

    All interface design- e.g. the breakdown of logic into components like functions, classes, methods,
    i.e. not just interface in the OO interface, or interface in a specific language, but rather, in the broadest sense,
    the communication patterns of all modules in your program- can be thought of as designing a language- a DSL.
    This metaphor of interface as language, allows us to reason about incorrectness in code in terms of undefined behavior
    of the language. And this is what all manners of tests (unit, functional, end-to-end) are attempting to do, ensure
    that the program doesn't invalid states/"undefined behavior".

    Mostly, this interface, this DSL is not explicitly defined.

    One example of the above challenges, is working with pages of bytes in the context of btree. Now, since I'm reading
    writing subsets of bytes to larger byte strings, the interpreter can't directly help. And I need to either implement,
    a better lower level DSL, i.e. one that supports better debugging.

6/9
    When the codebase gets complex, all interfaces will show their leakiness. Leakiness will manifest in wanting to
    expose internal variables (e.g. propagating max key value when max key inserted in leaf up internal node ancestors;
    if the max value is not propagated, the internal node will have to look it up, which would be a replicated op- and this
    also speaks to the broader compute/storage tradeoff. ). The options are these: 1) break the abstraction, expose
    internals- makes the code brittle; 2) keep the abstraction, make the code less performant, 3) rethink the abstraction
    such that the internal to be exposed is a first-class exposed objects- more work, but keeps the code performant and
    abstractions sounds.

6/10
    I missed a crucial detail in my modelling of the insert algorithms, since I assumed some things about how a node
    would be split, and how the post-split key distribution will look like; this meant a lot of debugging. This is
    why it's very important to independently reason about the algorithm. Because, I kept persisting in that modelling error
    and so in the program all that was left to was fix the control flow, the +1 errors.

6/10
    There's a critical stress between core developement and auxilary tools development, i.e. broadly infra. Core is
    fundamentally the algorithm of the business. If it involves humans, it maybe better thought of the aggregate flows
    of the business. And there are things that don't have a direct value add, but are needed, e.g. testing. It's not
    a direct value add, but it is an indirect value add in reliability.

    But the infra should involve with the complexity of the core, e.g. insert keys into tree in sorted order;
    small tree - manually inspect state of tree is feasible, for medium tree- tests on expected values, for large trees-
    internal consistency checkers, combined with random tree generators. Here sizes are metaphors for complexity.

6/11
    An off by one error fixed here, and an inequality fixed there, and poof, you have a (semi-) working btree (only find, and
    insert; no delete). It's cool, how it all comes together. It's a very fascinating exercise to work with this pure
    level of abstraction, i.e. just the raw data structure, and some algorithm applied thereon. Aside about how complexity, and
    friction in the context of code arises: complexity arises either due to the inherent nature of the problem, or it arises
    due to the complexity of "integrations". This pure level of abstraction is where complexity arises entirely from the problem.
    Because there is no integration, e.g. the btree with 2000 lines has no non-system imports. And it's certainly a good
    thing to exercise your brain thus.

    The next step before implementing anything else, would be some code hygiene, e.g. move tree to a separate module. Assess,
    if any logic should be refactored, api's cleaned up. Add any more validations.

    Actually, the actual next step should be to reflect on whether to work on delete, learndb-rs, debugging/inner-loop.

6/17
    There are two crucial aspects to testing. The first is given this input, my system should return this output. And we
    can think about these input/output, more abstractly to include metrics like SLA, e.g. the calculator should solve 2+2
    in under 10ms. These are the known knowns. The second aspect of testing, is the known unknowns, and the unknown unknown.
    A known unknown maybe the exact concurrency limit (curve) of a system. The known unknowns are the un-measured, or the
    un-modelled aspects of the system. These could with more effort be made more known, but never perfectly so.
    A unknown unknown is a completely unknown behavior of your system. But it's always grounded in some assumption. In the
    most extreme case, consider a beta-particle hitting your memory, and flipping a bit or two in an undetectable way. Now
    this is a very rare, scenario, but your assumptions that integers are immutable is valid insofar, as your physical
    memory is perfectly isolated.

    This distinction is important, because the unknown aspects of the systems, can cause failure. One way to discover the
    unknowns is through randomized searches, through your program's state space.

6/28
    A case study in evolving code. I initially started with the assumption that trees can never be unary. This simplified
    some operations, e.g. find; and it was not possible for a (sub-)tree to become unary from insert ops. When I
    started implementing delete/restructure- I faced the choice of either allow unary trees and update the existing ops
    to handle a unary tree. Or guard against the tree ending up in such a state. I opted for this initially because
    it seemed easier to restrict the tree not becoming unary. But then this started getting more hairy; for example- some
    delete ops, could lead to unary node; and you would need many conditions in delete; restructure; and find and possibly
    even some rebalancing (e.g. if a node's child cells are all deleted).

    One take away from this is that software that's less piece-meal and more of a consistent and well-thought abstraction is
    more likely to generalize in it's use cases; and in likely to be less complex in its logic.


6/29
    I'm doing a fairly crude compaction (left most; without any buffer). In practice, this should be configurable, and
    perhaps at least not so eager.

7/2
    internal_num_keys vs. internal_num_children.
    internal_num_keys is the accessor for the number of keys on an internal node; This is very similar
    to leaf_num_keys; with the crucial difference, that internal node have +1 (right) children than keys, while leaf
    node have the same number of cells- and this is because the two node types have different functions.

    This makes delete tricky, because it makes the interpretation of number of children on a node, ambiguous. This
    itself is related to the underlying modeling of the tree, and whether unary is a valid tree etc. Specifically,
    internal_num_cells == 0 could be a unary or zeroary tree. So delete has to track the state of tree before
    and determine which of these two buckets it's in. A clearer way would be to use a new accessor internal_num_children.
    Ideally, this would have include the total count; and could easily disambiguate unary and zeroary calls. But this
    is where the ideal design runs into the weight of the existing work; specifically, my options are: 1) introduce a new field-
    wastes space; 2) update internal_num_keys to internal_num_children but this is tricky because its a very commonly used
    method, including in find, binary search etc; but anyways this is not an excuse;

    The point being, somewhere in the deep of the mist, you will realize the weakest aspects of your design.

    Anyways for now, will opt for contextual delete, i.e. no new fields?

7/5
    One challenge of memory vs on-disk data structures is that all memory protections, etc. are gone, since they
    only exist for in memory object. For on-disk objects, all of that: memory management, resource management, has to
    be one by oneself. And more importantly, it depends on the correctness of other parts of the system.


11/17
    PEGs vs hand-written recursive descent parser
    Working on extending the database to support a more complex grammar/DDL/DQL/DDM to support more complex functionality,
    e.g. filtering on search. Initially, I experimented with a PEG (parse expression grammar) library. PEGs are
    quiet expressive, and easy to use. But PEGs are limited since they generally expect the input to be correctly
    formatted- and as such aren't very good at reporting errors, and more broadly, doing meaningful things with
    invalid input.

    Then I created a recursive descent parser using the skeleton from crafting interpreters by Robert Nystrom.

11/18
    Python vs. statically typed language
    I started with python because it seemed the fastest. But now given the complexity, and need to refactor code- I feel
    a static language would have been better. Actually, this is more nuanced- python is useful in bootstrapping really
    quickly- 2 lines for a repl. And a lot of the benefits of statically typed languages can be simulated to
    a certain level, e.g. mypy + type annotations ~ static type system. But it's certainly less than a industrial grade
    statically typed language, and this seems to be made worse with the advantage statically typed language in IDEs.
    Again, there seems to be much advancement in editors- but even things as refactor don't feel as fluid.
    Will stick with python (or perhaps later migrate some stuff to a diff lang) But something to reflect on.

11/18
    I'm trying to add support for some DDL/M/Q support. What I have so far can be considered a template. It's functionality
    is the ability to store integer keys and support efficient  ops (find, insert, delete). Next, I want to support dynamic,
    multiple tables. This will require a host of changes, across the entire project.
    The first step of this is the parser, which is complete. The details are in documented in
    docs/to_multiple_dynamic_tables.txt


11/23
    In trying to support arbitrary schemas, one must consider how a logical schema is represented; and how are
    ops on schemas supported. This includes validation and generation of schema.
    One must also consider the type system and how datatypes are implemented and manipulated
    at runtime and stored on disk. This also is a can of worms, because they're are many tradeoffs around
    how the data is serialized (e.g. fixed length vs. variable length encodings). Another concern is, whether
    null values are explicitly stored, i.e. sparse vs non-sparse representations.

    Actually, we need to store the schema anyways, since any type can be null'ed. See (docs/storing-diff-datatypes.txt)
    for more details.

    Further deep, there may be tradeoffs around consistency/validity/expressivity of the type system.

11/24
    The serialized bytes (persisted to disk) need to encoding many details, e.g. column data type etc. This information
    goes in the header. The btree module was doing header read/write by referring to an implicit header defined via
    constants. This was quite manual.

    I am considering a higher level interface for reading, writing the header. This would also make it easier to do
    validations and change constants. Ideally the interface is also self documenting.

    e.g.

    Header has many fields.
    Each field has:
        - a name (e.g. is_root_node), and
        - value (which can be optionally mapped to an enum type)
        - repeat count [0, inf]

    header = Header()
    # add a new field, with a given size
    header.append_field('header_size', type=int, size=4)
    # add a new field that's repeated
    header.append_field('serial_type', type=SerialType, size=4)

    # alt. syntax
    header_size = Field(name='header_size', type=int, size=4)
    stype = Field(name='serial_type', type=SerialType, size=4)
    var_len = OptionalField(name='length', type=int, size=4, read_if=lambda st: st.value.type = Text)

    if stype.value
    header.append(header_size)

    stype_len_pair = Field(Field(name='serial_type', type=SerialType, size=4),
                            Field(name='length', type=int, size=4, repeater=ZeroOrOne), repeater='', repeat_until='end')
    header.append(stype_len_pair)

    # the header should return a parser
    parser = header.get_parser()

    => {header_size: 4, columns: [{serial_type: 4, length: 32}] }

    ---

    What's peculiar here is that we end up with another implicit parser/language. This is distinct from the sql parser, i.e.
    the front end. This is a parser for the contents of the on-disk node. What's common between the 2 languages is that structure
    of the tokens encodes the meanings of the tokens. I suppose any time we have this- we need a parser.

    ---

    Actually, after experimenting a bit, it seems this header handler that can simultaneously be a self-documenting header
    definition as well as provide an auto-generated parser- will be quiet difficult to implement.

11/26
    Logically segmenting components is tricky. The first level division is between stateless and stateful
    ops. This is broadly, the split between virtual machine and state manager. But there are a lot of boundary concerns, e.g.
    how a record should be created. Further, there is a fundamental tradeoffs between encapsulating state (object-oriented
    perspective and clearly expressing compute on some data (functional perspective).

    One case in point is who should be responsible for initializing the catalog. At one level, I want the VM
    to handle all reads/writes since logic will be tricky and best contained in one object. However, then the
    VM would have to intimately understand the catalog format, and this would make the VM fatter.

11/28
    See discussion on updating btree to support variable length cells in docs/btree-variable-length-cells.txt

11/29
    Incrementally evolving software

    One issue I'm running into is whether the API around leaf nodes should expect a key of type integer or bytes.
    The reason is, for now I'm doing word-sized integer keys. But eventually I will want to support variable length integer
    keys (any byte string can be interpreted as a multi-bytes int). So there is some api (new, forward looking for leaf nodes)
    which expects byte strings as keys. And older api (which I'm trying to reuse for internal nodes).
    But mixing the old and new apis can make the code confusing. But the alternative is either:
    a much bigger one-shot change, i.e. update old reuseable api to expect bytes, or a slower two step update, i.e.
    support new file format with fixed sized ints. Then update to support variable sized ints.

    But this is just a place where competing objectives collide- time, code clarity/correctness. These two are not necessarily
    in conflict. Higher code clarity, makes it less like to have bugs, and easier to debug, and you need a minimum level of code
    beauty (i.e. everything that's not raw functional, i.e. the difference between a turing machine and a slick interface). But
    beyond a certain level- the gain in productivity is limited.


